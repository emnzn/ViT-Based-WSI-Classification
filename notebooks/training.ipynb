{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "import os\n",
    "from typing import Dict, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from utils import swin_transformer, WSIDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>grade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16425</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16421</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16223</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16089</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16026</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id grade\n",
       "0  16425     1\n",
       "1  16421     1\n",
       "2  16223     1\n",
       "3  16089     1\n",
       "4  16026     1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dir = os.path.join(\"..\", \"data\", \"labels.csv\")\n",
    "\n",
    "labels = pd.read_csv(label_dir)\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id       0\n",
       "grade    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = labels[\"id\"].tolist()\n",
    "grades = labels[\"grade\"].map(lambda x: 0 if x == \"1\" else 1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'16425': 0,\n",
       " '16421': 0,\n",
       " '16223': 0,\n",
       " '16089': 0,\n",
       " '16026': 0,\n",
       " '15779': 0,\n",
       " '15513': 1,\n",
       " '15463A': 1,\n",
       " '15463B': 1,\n",
       " '15286': 1,\n",
       " '15060': 1,\n",
       " '15050': 0,\n",
       " '14917A': 1,\n",
       " '14917B': 1,\n",
       " '14917C': 1,\n",
       " '14732A': 1,\n",
       " '14732B': 1,\n",
       " '14703': 1,\n",
       " '14696A': 0,\n",
       " '14696B': 1,\n",
       " '14572A': 1,\n",
       " '14404': 0,\n",
       " '14148': 1,\n",
       " '14120': 0,\n",
       " '14080': 0,\n",
       " '14077': 0,\n",
       " '13982': 0,\n",
       " '13838': 0,\n",
       " '13820': 0,\n",
       " '13791': 1,\n",
       " '13742': 0,\n",
       " '13675': 0,\n",
       " '13663': 0,\n",
       " '13645': 0,\n",
       " '13554': 0,\n",
       " '13540': 0,\n",
       " '13475': 1,\n",
       " '13424': 0,\n",
       " '13353': 0,\n",
       " '13284': 0,\n",
       " '13267': 0,\n",
       " '13193': 1,\n",
       " '13191': 1,\n",
       " '13179': 0,\n",
       " '13153': 1,\n",
       " '13119': 0,\n",
       " '13055': 1,\n",
       " '13054A': 0,\n",
       " '13054B': 1,\n",
       " '12925': 0,\n",
       " '12904': 1,\n",
       " '12845': 1,\n",
       " '12801': 1,\n",
       " '12768': 0,\n",
       " '12691': 0,\n",
       " '12662': 1,\n",
       " '12615': 0,\n",
       " '12570': 0,\n",
       " '12529': 0,\n",
       " '12524': 1,\n",
       " '12447': 0,\n",
       " '12424': 0,\n",
       " '12404': 0,\n",
       " '12399': 0,\n",
       " '12380': 0,\n",
       " '12327': 1,\n",
       " '12220': 0,\n",
       " '12186': 0,\n",
       " '12180': 0,\n",
       " '12169': 0,\n",
       " '12145': 0,\n",
       " '12120': 0,\n",
       " '12063': 0,\n",
       " '12015': 0,\n",
       " '12010A': 1,\n",
       " '11987': 0,\n",
       " '11943': 0,\n",
       " '11938': 0,\n",
       " '11929': 1,\n",
       " '11871': 1,\n",
       " '11845': 0,\n",
       " '11820': 1,\n",
       " '11797': 0,\n",
       " '11785C': 0,\n",
       " '11783': 1,\n",
       " '11775': 0,\n",
       " '11753': 0,\n",
       " '11749': 1,\n",
       " '11727': 1,\n",
       " '11651': 0,\n",
       " '11575': 1,\n",
       " '11541': 0,\n",
       " '11539': 0,\n",
       " '11481': 0,\n",
       " '11479': 0,\n",
       " '11464': 0,\n",
       " '11422': 0,\n",
       " '11420': 0,\n",
       " '11402': 1,\n",
       " '11389': 1,\n",
       " '11329': 1,\n",
       " '11320': 0,\n",
       " '11303': 1,\n",
       " '11293': 0,\n",
       " '11287': 0,\n",
       " '11226': 0,\n",
       " '11224': 1,\n",
       " '11189': 1,\n",
       " '11170': 0,\n",
       " '11141': 0,\n",
       " '11043': 1,\n",
       " '11000': 0,\n",
       " '10934': 0,\n",
       " '10933': 0,\n",
       " '10902': 1,\n",
       " '10825': 1,\n",
       " '10773': 1,\n",
       " '10725': 0,\n",
       " '10659': 0,\n",
       " '10596': 1,\n",
       " '10576': 0,\n",
       " '10565': 0,\n",
       " '10548': 0,\n",
       " '10490': 1,\n",
       " '10336': 0,\n",
       " '10252': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = {patient_id: grade for patient_id, grade in zip(ids, grades)}\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"..\", \"data\", \"UNI\", \"trial-1\")\n",
    "label_dir = os.path.join(\"..\", \"data\", \"labels.csv\")\n",
    "\n",
    "train_path = os.path.join(data_dir, \"train\")\n",
    "val_path = os.path.join(data_dir, \"val\")\n",
    "test_path = os.path.join(data_dir, \"test\")\n",
    "\n",
    "all_paths = [os.path.join(train_path, file) for file in os.listdir(train_path)] + \\\n",
    "    [os.path.join(val_path, file) for file in os.listdir(val_path)] + [os.path.join(test_path, file) for file in os.listdir(test_path)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[350, 224]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_shapes = [np.load(file).shape for file in all_paths]\n",
    "\n",
    "max_height = max(embedding_shapes, key=lambda x: x[0])[0]\n",
    "max_width = max(embedding_shapes, key=lambda x: x[1])[1]\n",
    "\n",
    "target_shape = [max_height, max_width]\n",
    "\n",
    "target_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_embedding(embedding: torch.Tensor, target_shape: Tuple[int]) -> np.ndarray:\n",
    "\n",
    "    \"\"\"\n",
    "    Pads the embedding to a target shape.\n",
    "    \"\"\"\n",
    "\n",
    "    current_shape = embedding.shape[1:]\n",
    "    print(current_shape)\n",
    "\n",
    "    delta_h = target_shape[0] - current_shape[0]\n",
    "    delta_w = target_shape[1] - current_shape[1]\n",
    "\n",
    "    pad_top = delta_h // 2\n",
    "    pad_bottom = delta_h - pad_top\n",
    "    \n",
    "    pad_left = delta_w // 2\n",
    "    pad_right = delta_w - pad_left\n",
    "\n",
    "    m = torch.nn.ZeroPad2d(padding=(pad_left, pad_right, pad_top, pad_bottom))\n",
    "\n",
    "    padded_embedding = m(embedding)\n",
    "\n",
    "    return padded_embedding\n",
    "\n",
    "\n",
    "# def pad_embedding(\n",
    "#     self, \n",
    "#     embedding: np.ndarray, \n",
    "#     target_shape: Tuple[int]\n",
    "#     ) -> np.ndarray:\n",
    "\n",
    "#     \"\"\"\n",
    "#     Pads the embedding to a target shape.\n",
    "#     \"\"\"\n",
    "\n",
    "#     current_shape = embedding.shape[:2]\n",
    "\n",
    "#     delta_h = target_shape[0] - current_shape[0]\n",
    "#     delta_w = target_shape[1] - current_shape[1]\n",
    "\n",
    "#     pad_top = delta_h // 2\n",
    "#     pad_bottom = delta_h - pad_top\n",
    "    \n",
    "#     pad_left = delta_w // 2\n",
    "#     pad_right = delta_w - pad_left\n",
    "\n",
    "#     padded_img = np.pad(\n",
    "#         embedding,\n",
    "#         pad_width=((pad_top, pad_bottom), (pad_left, pad_right), (0, 0)),\n",
    "#         mode=\"constant\",\n",
    "#         constant_values=0.0\n",
    "#     )\n",
    "\n",
    "#     return padded_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 302, 209])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_embedding = torch.tensor(np.load(all_paths[0]))\n",
    "test_embedding = test_embedding.permute(2, 0, 1) # [channels, height, width]\n",
    "\n",
    "test_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([302, 209])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 350, 224])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_padded = pad_embedding(test_embedding, target_shape)\n",
    "\n",
    "test_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WSIDataset(Dataset):\n",
    "\n",
    "    \"\"\"\n",
    "    Creates the dataset class for the dataloader.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir: str\n",
    "        The directory to the embeddings.\n",
    "    \n",
    "    label_dir: str\n",
    "        The directory to the labels.\n",
    "\n",
    "    mil: bool\n",
    "        Whether compiling for a Multiple-Instance Based Model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    embedding: torch.Tensor\n",
    "        The embedding of the WSI given a foundation model.\n",
    "    \n",
    "    label: str\n",
    "        The grade of the patient at the given datapoint.\n",
    "\n",
    "    patient_id: str\n",
    "        The patient id.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        data_dir: str, \n",
    "        label_dir: str,\n",
    "        mil: bool,\n",
    "        pad: bool,\n",
    "        target_shape: int\n",
    "        ):\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.filenames = os.listdir(data_dir)\n",
    "        self.labels = self.generate_labels(label_dir)\n",
    "        self.mil = mil\n",
    "        self.pad = pad\n",
    "        self.target_shape = target_shape\n",
    "\n",
    "        assert all([Path(i).stem in self.labels for i in self.filenames]), \"All patient ids must have a label\"\n",
    "\n",
    "    \n",
    "    def generate_labels(self, label_dir: str) -> Dict[str, str]:\n",
    "\n",
    "        \"\"\"\n",
    "        Creates a dictionary containing the patient ids as keys\n",
    "        and the associated Meningioma grade as the values.\n",
    "        \"\"\"\n",
    "\n",
    "        labels = pd.read_csv(label_dir)\n",
    "        ids = labels[\"id\"].tolist()\n",
    "        grades = labels[\"grade\"].map(lambda x: 0 if x == \"1\" else 1).tolist()\n",
    "\n",
    "        labels = {patient_id: grade for patient_id, grade in zip(ids, grades)}\n",
    "\n",
    "        return labels\n",
    "\n",
    "    def pad_embedding(\n",
    "        self, \n",
    "        embedding: torch.Tensor, \n",
    "        target_shape: Tuple[int]\n",
    "        ) -> np.ndarray:\n",
    "\n",
    "        \"\"\"\n",
    "        Pads the embedding to a target shape.\n",
    "        The tensor must be of shape [C, H, W]\n",
    "        \"\"\"\n",
    "\n",
    "        current_shape = embedding.shape[1:]\n",
    "\n",
    "        delta_h = target_shape[0] - current_shape[0]\n",
    "        delta_w = target_shape[1] - current_shape[1]\n",
    "\n",
    "        pad_top = delta_h // 2\n",
    "        pad_bottom = delta_h - pad_top\n",
    "        \n",
    "        pad_left = delta_w // 2\n",
    "        pad_right = delta_w - pad_left\n",
    "\n",
    "        m = torch.nn.ZeroPad2d(padding=(pad_left, pad_right, pad_top, pad_bottom))\n",
    "\n",
    "        padded_embedding = m(embedding)\n",
    "\n",
    "        return padded_embedding\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        patient_id = Path(filename).stem\n",
    "        label = self.labels[patient_id]\n",
    "\n",
    "        embedding_path = os.path.join(self.data_dir, filename)\n",
    "        embedding = torch.tensor(np.load(embedding_path)).permute(2, 0, 1) # [channels, height, width]\n",
    "\n",
    "        if self.pad:\n",
    "            embedding = self.pad_embedding(embedding, self.target_shape)\n",
    "\n",
    "        if self.mil:\n",
    "            channels, height, width = embedding.shape\n",
    "            embedding = embedding.permute(1, 2, 0).reshape(height * width, channels)\n",
    "\n",
    "        return embedding, label, patient_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(\"..\", \"data\", \"UNI\", \"trial-1\", \"train\")\n",
    "label_dir = os.path.join(\"..\", \"data\", \"labels.csv\")\n",
    "\n",
    "wsi_dataset = WSIDataset(data_dir, label_dir, mil=False, pad=True, target_shape=target_shape)\n",
    "wsi_loader = DataLoader(wsi_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding, label, patient_id = next(iter(wsi_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 350, 224])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"v1\"\n",
    "variant = \"tiny\"\n",
    "num_classes = 2\n",
    "\n",
    "model = swin_transformer(version, variant, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinTransformer(\n",
       "  (features): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(1024, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): Permute()\n",
       "      (2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "          (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "          (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.018181818181818184, mode=row)\n",
       "        (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): PatchMerging(\n",
       "      (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.03636363636363637, mode=row)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.05454545454545456, mode=row)\n",
       "        (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): PatchMerging(\n",
       "      (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.07272727272727274, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.09090909090909091, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.10909090909090911, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1272727272727273, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.14545454545454548, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.16363636363636364, mode=row)\n",
       "        (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): PatchMerging(\n",
       "      (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "      (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.18181818181818182, mode=row)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.2, mode=row)\n",
       "        (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  (permute): Permute()\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (head): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0117, -0.0284]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 278, 194])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('14917B',)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(embedding).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "histology",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
